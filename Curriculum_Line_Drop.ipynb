{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Curriculum_Line_Drop.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VouTljcpLD-A",
        "colab_type": "text"
      },
      "source": [
        "# Installing desired version of fastai - pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0Tf08U0r5GP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install fastai==1.0.54 > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VmFz6IJKM45",
        "colab_type": "text"
      },
      "source": [
        "# Setting up Environment to get reproducible results\n",
        "\n",
        "[** Didn't set num_workers as 1]\n",
        "\n",
        "Ref : https://docs.fast.ai/dev/test.html#getting-reproducible-results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APQjxSSkKKXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 42\n",
        "\n",
        "# python RNG\n",
        "import random\n",
        "random.seed(seed)\n",
        "\n",
        "# pytorch RNGs\n",
        "import torch\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# numpy RNG\n",
        "import numpy as np\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv4c2zj4J0fG",
        "colab_type": "text"
      },
      "source": [
        "# Loading required packages\n",
        "\n",
        "(Some packages can be removed, as I have carried out other operations that are not part of this release)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZCzCpUDsTsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import shutil as sh\n",
        "import os\n",
        "\n",
        "from fastai.layers import *\n",
        "from fastai.torch_core import *\n",
        "\n",
        "from fastai.vision import *\n",
        "from fastai.callbacks.csv_logger import CSVLogger\n",
        "from fastai.callbacks.hooks import *\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "from fastai.torch_core import *\n",
        "from fastai.callback import *\n",
        "from fastai.callbacks import *\n",
        "from fastai.basic_train import *\n",
        "from fastai.basic_data import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REEHRBQmLKbz",
        "colab_type": "text"
      },
      "source": [
        "# Downloading CIFAR100 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyz_iBnxsYcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = untar_data(URLs.CIFAR_100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co1a_MdlLUVR",
        "colab_type": "text"
      },
      "source": [
        "# Parameters and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ8em3bksbBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs = 128\n",
        "epochs = 50\n",
        "metrics = [accuracy, error_rate, top_k_accuracy]\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "base_lr = 0.1\n",
        "momentum= 0.9\n",
        "w_decay = 0.0001\n",
        "\n",
        "opt_func = partial (optim.SGD, momentum=0.9)\n",
        "\n",
        "result_path = '/content/sample_data/activation/results/'\n",
        "final_path = \"/content/drive/'My Drive'/activation/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfADxASeskWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds_tfms = ([ *rand_pad(4, 32), flip_lr(p=0.5)], [])\n",
        "data = ImageDataBunch.from_folder(path, valid='test', bs=bs ,ds_tfms = ds_tfms , val_bs =100).normalize(cifar_stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCD1ZZRXLdB9",
        "colab_type": "text"
      },
      "source": [
        "# Modified NIN network\n",
        "\n",
        "Official Implementation\n",
        "\n",
        "https://gist.github.com/mavenlin/e56253735ef32c3c296d#file-solver-prototxt\n",
        "\n",
        "Our implementation is based on \n",
        "\n",
        "https://github.com/jiecaoyu/pytorch-nin-cifar10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQyEdkqpsrI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.num_classes = 100\n",
        "    self.classifier = nn.Sequential(\n",
        "                      nn.Conv2d(3, 192, kernel_size=5, stride=1, padding=2),\n",
        "                      nn.ReLU(inplace=True),\n",
        "        \n",
        "                      nn.Conv2d(192, 160, kernel_size=1, stride=1, padding=0),\n",
        "                      nn.ReLU(inplace=True),\n",
        "        \n",
        "                      nn.Conv2d(160,  96, kernel_size=1, stride=1, padding=0),\n",
        "                      nn.ReLU(inplace=True),\n",
        "        \n",
        "                      nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "                      nn.Dropout(0.5),\n",
        "\n",
        "                      nn.Conv2d(96, 192, kernel_size=5, stride=1, padding=2),\n",
        "                      nn.ReLU(inplace=True),\n",
        "        \n",
        "                      nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
        "                      nn.ReLU(inplace=True),\n",
        "        \n",
        "                      nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
        "                      nn.ReLU(inplace=True),\n",
        "        \n",
        "                      nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
        "                      nn.Dropout(0.5),\n",
        "\n",
        "                      nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
        "                      nn.ReLU(inplace=True),\n",
        "        \n",
        "                      nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
        "                      nn.ReLU(inplace=True),\n",
        "        \n",
        "                      nn.Conv2d(192,  self.num_classes , kernel_size=1, stride=1, padding=0),\n",
        "                      nn.ReLU(inplace=True),\n",
        "        \n",
        "                      nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n",
        "                      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.classifier(x)\n",
        "    x = x.view(x.size(0), self.num_classes)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1jIaKI1lmiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Rank_Activations(HookCallback):\n",
        "    \"Callback that record the mean and std of activations.\"\n",
        "    stats= []\n",
        "    \n",
        "    def on_train_begin(self, **kwargs):\n",
        "        \"Initialize stats.\"\n",
        "        super().on_train_begin(**kwargs)\n",
        "        self.stats = []\n",
        "        self.num = 0\n",
        "\n",
        "    def hook(self, m:nn.Conv2d, i:Tensors, o:Tensors)->Tuple[Rank0Tensor,Rank0Tensor]:\n",
        "        \"Take the mean and std of `o`.\"\n",
        "        self.num = self.num + 1\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "          res = o.mean().item()\n",
        "          return res\n",
        "          #print(m)\n",
        "        else:\n",
        "          #res = None\n",
        "          pass\n",
        "      \n",
        "    def on_batch_end(self, train, **kwargs):\n",
        "        \"Take the stored results and puts it in `self.stats`\"\n",
        "        if (not train): self.stats.append(self.hooks.stored)\n",
        "          \n",
        "    def on_train_end(self, **kwargs):\n",
        "        \"Polish the final result.\"\n",
        "        super().on_train_end(**kwargs)\n",
        "        self.stats = (self.stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObXs8cj2L5Io",
        "colab_type": "text"
      },
      "source": [
        "# Initializing weights and bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcYid_IfMHzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net()\n",
        "# model.cuda()\n",
        "\n",
        "pretrained=False\n",
        "if pretrained:\n",
        "    params = pickle.load(open('data/params', 'r'))\n",
        "    index = -1\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            index = index + 1\n",
        "            weight = torch.from_numpy(params[index])\n",
        "            m.weight.data.copy_(weight)\n",
        "            index = index + 1\n",
        "            bias = torch.from_numpy(params[index])\n",
        "            m.bias.data.copy_(bias)\n",
        "else:\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            m.weight.data.normal_(0, 0.05)\n",
        "            m.bias.data.normal_(0, 0.0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMfiYInuMOXv",
        "colab_type": "text"
      },
      "source": [
        "# Splitting Model into sections for discriminative learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8lfuSKOaVUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = Learner(data, model, loss_func= loss_func, opt_func = opt_func ,metrics=metrics).mixup()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBs1iGqzsuKv",
        "colab_type": "code",
        "outputId": "8d26ae15-ca90-40cd-ec94-1eaa6b938b4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learn.split([[ learn.model.classifier[0:20]], [learn.model.classifier[20] ]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Learner(data=ImageDataBunch;\n",
              "\n",
              "Train: LabelList (50000 items)\n",
              "x: ImageList\n",
              "Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32)\n",
              "y: CategoryList\n",
              "flatfish,flatfish,flatfish,flatfish,flatfish\n",
              "Path: /root/.fastai/data/cifar100;\n",
              "\n",
              "Valid: LabelList (10000 items)\n",
              "x: ImageList\n",
              "Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32)\n",
              "y: CategoryList\n",
              "flatfish,flatfish,flatfish,flatfish,flatfish\n",
              "Path: /root/.fastai/data/cifar100;\n",
              "\n",
              "Test: None, model=Net(\n",
              "  (classifier): Sequential(\n",
              "    (0): Conv2d(3, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): ReLU(inplace)\n",
              "    (2): Conv2d(192, 160, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (3): ReLU(inplace)\n",
              "    (4): Conv2d(160, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (5): ReLU(inplace)\n",
              "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (7): Dropout(p=0.5)\n",
              "    (8): Conv2d(96, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (9): ReLU(inplace)\n",
              "    (10): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (11): ReLU(inplace)\n",
              "    (12): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (13): ReLU(inplace)\n",
              "    (14): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
              "    (15): Dropout(p=0.5)\n",
              "    (16): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace)\n",
              "    (18): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (19): ReLU(inplace)\n",
              "    (20): Conv2d(192, 100, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (21): ReLU(inplace)\n",
              "    (22): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.sgd.SGD'>, momentum=0.9), loss_func=CrossEntropyLoss(), metrics=[<function accuracy at 0x7f825ae7d048>, <function error_rate at 0x7f825ae7d268>, <function top_k_accuracy at 0x7f825ae7d158>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/.fastai/data/cifar100'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), functools.partial(<class 'fastai.callbacks.mixup.MixUpCallback'>, alpha=0.4, stack_x=False, stack_y=True)], callbacks=[], layer_groups=[Sequential(\n",
              "  (0): Sequential(\n",
              "    (0): Conv2d(3, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): ReLU(inplace)\n",
              "    (2): Conv2d(192, 160, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (3): ReLU(inplace)\n",
              "    (4): Conv2d(160, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (5): ReLU(inplace)\n",
              "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (7): Dropout(p=0.5)\n",
              "    (8): Conv2d(96, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (9): ReLU(inplace)\n",
              "    (10): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (11): ReLU(inplace)\n",
              "    (12): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (13): ReLU(inplace)\n",
              "    (14): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
              "    (15): Dropout(p=0.5)\n",
              "    (16): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace)\n",
              "    (18): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (19): ReLU(inplace)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): Conv2d(192, 100, kernel_size=(1, 1), stride=(1, 1))\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnLRNWL9MToc",
        "colab_type": "text"
      },
      "source": [
        "# Decalring Step Dropout as Callback\n",
        "\n",
        "[Not used in this notebook]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byf9ZSraA5MZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Drop_Activations(LearnerCallback):\n",
        "  train_flag = False\n",
        "  \n",
        "  def on_train_begin(self, **kwargs):\n",
        "    \"Initialize stats.\"\n",
        "    super().on_train_begin(**kwargs)\n",
        "    self.train_flag = True\n",
        "    \n",
        "  def on_epoch_begin(self, **kwargs):\n",
        "    if (self.train_flag):\n",
        "      epoch_num = kwargs['epoch']\n",
        "      epoch_tot = kwargs['n_epochs']\n",
        "\n",
        "      child = children(self.model)[0]\n",
        "      for item in child:\n",
        "        if isinstance(item, nn.Dropout):\n",
        "          item.p = (epoch_num  / epoch_tot) * 0.5\n",
        "\n",
        "  def on_train_end(self, **kwargs):\n",
        "    \"Initialize stats.\"\n",
        "    super().on_train_begin(**kwargs)\n",
        "    self.train_flag = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXJuxCSPu7AH",
        "colab_type": "text"
      },
      "source": [
        "# Declaring Curriculum Line  dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTiJDMFIhkNZ",
        "colab_type": "code",
        "outputId": "2c6110e7-5339-4d49-96a4-7fcddbf04c40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# Params as per paper\n",
        "\"\"\" \n",
        "keep_prob_input: _prob_input,   #0.9\n",
        "keep_prob_conv : _prob_conv,    #0.75\n",
        "keep_prob_fc   : _prob_fc       #0.5\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "gamma = 0.001\n",
        "p = 0.5 \n",
        "\n",
        "\n",
        "def line_drop(x):\n",
        "  return (x / (40 * 390)) * p\n",
        "\n",
        "def schedule_dropout(x):\n",
        "    val = (1.0 - p) * np.exp(- gamma * x ) + p\n",
        "    return val\n",
        "\n",
        "#     val = -(1.-p)* np.exp(- gamma * x) + 1 # Default. \n",
        "#     This is value set to keep_prob in TF. \n",
        "#     In case of pytorch it refers to probability to make zero\n",
        "def Anneal_dropout(x):\n",
        "    val =    (1.-p)* np.exp(- gamma * x)\n",
        "    return val\n",
        "\n",
        "# 40  refers to number of epochs\n",
        "# 390 refers to (number of train images / (batch_size))\n",
        "j = []\n",
        "for i in range(0,40 * 390):  \n",
        "  j.append(line_drop(i))\n",
        "\n",
        "num_iter = 390\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(j)\n",
        "plt.ylabel('some numbers')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MumbqAEO11hN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Curriculum_Dropout(LearnerCallback):\n",
        "    \"Curriculum Dropout Callback\"\n",
        "    count = 0\n",
        "    train_flag = False\n",
        "    \n",
        "    def on_train_begin(self, **kwargs):\n",
        "        \"Initialize stats.\"\n",
        "        super().on_train_begin(**kwargs)\n",
        "        self.train_flag = True\n",
        "        self.count = 0\n",
        "\n",
        "    def on_batch_begin(self, train, **kwargs):\n",
        "        \"Take the stored results and puts it in `self.stats`\"\n",
        "        if (train): \n",
        "          self.count += 1; \n",
        "          child = children(self.model)[0]\n",
        "          for item in child:\n",
        "            if isinstance(item, nn.Dropout):\n",
        "              item.p = line_drop(kwargs['iteration'])\n",
        "          \n",
        "    def on_train_end(self, **kwargs):\n",
        "        \"Polish the final result.\"\n",
        "        super().on_train_end(**kwargs)\n",
        "        self.count = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3usmfeiZMu31",
        "colab_type": "text"
      },
      "source": [
        "# Using the curriculum line drop in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7GkZ8ajMqjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learn.callbacks += [ Drop_Activations(learn) ]\n",
        "learn.callbacks += [ Rank_Activations(learn) ]\n",
        "learn.callbacks += [ Curriculum_Dropout(learn) ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5fz5C-KM3On",
        "colab_type": "text"
      },
      "source": [
        "# Running the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P48QhLSTKQWL",
        "colab_type": "code",
        "outputId": "1f35ffad-796f-40a6-8fea-62d384fba055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs =40\n",
        "learn.fit( epochs ,  lr= (1 * base_lr , 0.1 * base_lr) , wd = (2 * w_decay , 1 *w_decay))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>error_rate</th>\n",
              "      <th>top_k_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.457010</td>\n",
              "      <td>4.407050</td>\n",
              "      <td>0.038200</td>\n",
              "      <td>0.961800</td>\n",
              "      <td>0.151000</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.269063</td>\n",
              "      <td>4.121736</td>\n",
              "      <td>0.076700</td>\n",
              "      <td>0.923300</td>\n",
              "      <td>0.247100</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.116526</td>\n",
              "      <td>3.953439</td>\n",
              "      <td>0.095900</td>\n",
              "      <td>0.904100</td>\n",
              "      <td>0.302100</td>\n",
              "      <td>00:38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.971855</td>\n",
              "      <td>3.686150</td>\n",
              "      <td>0.132000</td>\n",
              "      <td>0.868000</td>\n",
              "      <td>0.375600</td>\n",
              "      <td>00:38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.844726</td>\n",
              "      <td>3.446853</td>\n",
              "      <td>0.165200</td>\n",
              "      <td>0.834800</td>\n",
              "      <td>0.437800</td>\n",
              "      <td>00:38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.688110</td>\n",
              "      <td>3.269913</td>\n",
              "      <td>0.190000</td>\n",
              "      <td>0.810000</td>\n",
              "      <td>0.487800</td>\n",
              "      <td>00:38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.562614</td>\n",
              "      <td>3.147001</td>\n",
              "      <td>0.217900</td>\n",
              "      <td>0.782100</td>\n",
              "      <td>0.520100</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.454859</td>\n",
              "      <td>3.004339</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.745000</td>\n",
              "      <td>0.567000</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.357899</td>\n",
              "      <td>2.804860</td>\n",
              "      <td>0.292600</td>\n",
              "      <td>0.707400</td>\n",
              "      <td>0.608800</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.217531</td>\n",
              "      <td>2.596706</td>\n",
              "      <td>0.342900</td>\n",
              "      <td>0.657100</td>\n",
              "      <td>0.659300</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.176359</td>\n",
              "      <td>2.565075</td>\n",
              "      <td>0.346900</td>\n",
              "      <td>0.653100</td>\n",
              "      <td>0.666600</td>\n",
              "      <td>00:38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>3.084729</td>\n",
              "      <td>2.545688</td>\n",
              "      <td>0.358000</td>\n",
              "      <td>0.642000</td>\n",
              "      <td>0.669500</td>\n",
              "      <td>00:38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>3.016130</td>\n",
              "      <td>2.387670</td>\n",
              "      <td>0.381900</td>\n",
              "      <td>0.618100</td>\n",
              "      <td>0.707000</td>\n",
              "      <td>00:38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.960802</td>\n",
              "      <td>2.257072</td>\n",
              "      <td>0.405400</td>\n",
              "      <td>0.594600</td>\n",
              "      <td>0.733600</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.898826</td>\n",
              "      <td>2.158250</td>\n",
              "      <td>0.441400</td>\n",
              "      <td>0.558600</td>\n",
              "      <td>0.743200</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.868570</td>\n",
              "      <td>2.169203</td>\n",
              "      <td>0.434500</td>\n",
              "      <td>0.565500</td>\n",
              "      <td>0.746100</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.812959</td>\n",
              "      <td>2.062223</td>\n",
              "      <td>0.459900</td>\n",
              "      <td>0.540100</td>\n",
              "      <td>0.765100</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.770391</td>\n",
              "      <td>2.020823</td>\n",
              "      <td>0.466400</td>\n",
              "      <td>0.533600</td>\n",
              "      <td>0.775200</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.740431</td>\n",
              "      <td>1.964042</td>\n",
              "      <td>0.474800</td>\n",
              "      <td>0.525200</td>\n",
              "      <td>0.785500</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.697144</td>\n",
              "      <td>1.977939</td>\n",
              "      <td>0.485400</td>\n",
              "      <td>0.514600</td>\n",
              "      <td>0.788800</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.677574</td>\n",
              "      <td>1.859682</td>\n",
              "      <td>0.497400</td>\n",
              "      <td>0.502600</td>\n",
              "      <td>0.802100</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.654670</td>\n",
              "      <td>1.873560</td>\n",
              "      <td>0.506400</td>\n",
              "      <td>0.493600</td>\n",
              "      <td>0.794900</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.612121</td>\n",
              "      <td>1.840981</td>\n",
              "      <td>0.511500</td>\n",
              "      <td>0.488500</td>\n",
              "      <td>0.807500</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.613979</td>\n",
              "      <td>1.809676</td>\n",
              "      <td>0.522500</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.810500</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.584736</td>\n",
              "      <td>1.799816</td>\n",
              "      <td>0.525500</td>\n",
              "      <td>0.474500</td>\n",
              "      <td>0.812200</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.571647</td>\n",
              "      <td>1.810198</td>\n",
              "      <td>0.519300</td>\n",
              "      <td>0.480700</td>\n",
              "      <td>0.804500</td>\n",
              "      <td>00:38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.520188</td>\n",
              "      <td>1.724252</td>\n",
              "      <td>0.544400</td>\n",
              "      <td>0.455600</td>\n",
              "      <td>0.819600</td>\n",
              "      <td>00:38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.544769</td>\n",
              "      <td>1.728637</td>\n",
              "      <td>0.538300</td>\n",
              "      <td>0.461700</td>\n",
              "      <td>0.823900</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>2.528942</td>\n",
              "      <td>1.723018</td>\n",
              "      <td>0.542900</td>\n",
              "      <td>0.457100</td>\n",
              "      <td>0.825200</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>2.534424</td>\n",
              "      <td>1.769100</td>\n",
              "      <td>0.542600</td>\n",
              "      <td>0.457400</td>\n",
              "      <td>0.821800</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.519965</td>\n",
              "      <td>1.724364</td>\n",
              "      <td>0.541700</td>\n",
              "      <td>0.458300</td>\n",
              "      <td>0.823100</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>2.513430</td>\n",
              "      <td>1.645889</td>\n",
              "      <td>0.554800</td>\n",
              "      <td>0.445200</td>\n",
              "      <td>0.834400</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>2.488781</td>\n",
              "      <td>1.665154</td>\n",
              "      <td>0.555000</td>\n",
              "      <td>0.445000</td>\n",
              "      <td>0.832000</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>2.477223</td>\n",
              "      <td>1.602351</td>\n",
              "      <td>0.571200</td>\n",
              "      <td>0.428800</td>\n",
              "      <td>0.844200</td>\n",
              "      <td>00:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>2.482057</td>\n",
              "      <td>1.634665</td>\n",
              "      <td>0.563800</td>\n",
              "      <td>0.436200</td>\n",
              "      <td>0.837300</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.479630</td>\n",
              "      <td>1.706516</td>\n",
              "      <td>0.545900</td>\n",
              "      <td>0.454100</td>\n",
              "      <td>0.824900</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>2.461889</td>\n",
              "      <td>1.627594</td>\n",
              "      <td>0.561800</td>\n",
              "      <td>0.438200</td>\n",
              "      <td>0.835100</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>2.476827</td>\n",
              "      <td>1.616358</td>\n",
              "      <td>0.568500</td>\n",
              "      <td>0.431500</td>\n",
              "      <td>0.840400</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>2.475356</td>\n",
              "      <td>1.599115</td>\n",
              "      <td>0.570000</td>\n",
              "      <td>0.430000</td>\n",
              "      <td>0.842100</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>2.479990</td>\n",
              "      <td>1.621674</td>\n",
              "      <td>0.564500</td>\n",
              "      <td>0.435500</td>\n",
              "      <td>0.837700</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWJcONPvur28",
        "colab_type": "text"
      },
      "source": [
        "# Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfjWz3CnJqTG",
        "colab_type": "code",
        "outputId": "b66a0ee0-b4f5-4842-d72d-f094dd5da26d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learn.validate()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.6216741, tensor(0.5645), tensor(0.4355), tensor(0.8377)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G8pHjiluDrk",
        "colab_type": "text"
      },
      "source": [
        "# Best Results\n",
        "\n",
        "[Obtained during trial run. I hope the variation is minimal across multiple runs]\n",
        "\n",
        "\n",
        ">  train_loss --------------------------------- 2.461889\n",
        "\n",
        "> valid_loss --------------------------------- 1.599115\n",
        "\n",
        "> accuracy ---------------------------------- 0.5712\n",
        "\n",
        "> error_rate --------------------------------- 0.4288\n",
        "\n",
        "> top_k_accuracy ----------------------- 0.8442\t"
      ]
    }
  ]
}