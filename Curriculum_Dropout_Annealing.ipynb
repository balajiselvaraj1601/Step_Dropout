{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Curriculum_Dropout_Annealing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VouTljcpLD-A",
        "colab_type": "text"
      },
      "source": [
        "# Installing desired version of fastai - pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0Tf08U0r5GP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install fastai==1.0.54 > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VmFz6IJKM45",
        "colab_type": "text"
      },
      "source": [
        "# Setting up Environment to get reproducible results\n",
        "\n",
        "[** Didn't set num_workers as 1]\n",
        "\n",
        "Ref : https://docs.fast.ai/dev/test.html#getting-reproducible-results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APQjxSSkKKXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 42\n",
        "\n",
        "# python RNG\n",
        "import random\n",
        "random.seed(seed)\n",
        "\n",
        "# pytorch RNGs\n",
        "import torch\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# numpy RNG\n",
        "import numpy as np\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv4c2zj4J0fG",
        "colab_type": "text"
      },
      "source": [
        "# Loading required packages\n",
        "\n",
        "(Some packages can be removed, as I have carried out other operations that are not part of this release)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZCzCpUDsTsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import shutil as sh\n",
        "import os\n",
        "\n",
        "from fastai.layers import *\n",
        "from fastai.torch_core import *\n",
        "\n",
        "from fastai.vision import *\n",
        "from fastai.callbacks.csv_logger import CSVLogger\n",
        "from fastai.callbacks.hooks import *\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "from fastai.torch_core import *\n",
        "from fastai.callback import *\n",
        "from fastai.callbacks import *\n",
        "from fastai.basic_train import *\n",
        "from fastai.basic_data import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REEHRBQmLKbz",
        "colab_type": "text"
      },
      "source": [
        "# Downloading CIFAR100 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyz_iBnxsYcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = untar_data(URLs.CIFAR_100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co1a_MdlLUVR",
        "colab_type": "text"
      },
      "source": [
        "# Parameters and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ8em3bksbBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs = 128\n",
        "epochs = 50\n",
        "metrics = [accuracy, error_rate, top_k_accuracy]\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "base_lr = 0.1\n",
        "momentum= 0.9\n",
        "w_decay = 0.0001\n",
        "\n",
        "opt_func = partial (optim.SGD, momentum=0.9)\n",
        "\n",
        "result_path = '/content/sample_data/activation/results/'\n",
        "final_path = \"/content/drive/'My Drive'/activation/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfADxASeskWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds_tfms = ([ *rand_pad(4, 32), flip_lr(p=0.5)], [])\n",
        "data = ImageDataBunch.from_folder(path, valid='test', bs=bs ,ds_tfms = ds_tfms , val_bs =100).normalize(cifar_stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCD1ZZRXLdB9",
        "colab_type": "text"
      },
      "source": [
        "# Modified NIN network\n",
        "\n",
        "Official Implementation\n",
        "\n",
        "https://gist.github.com/mavenlin/e56253735ef32c3c296d#file-solver-prototxt\n",
        "\n",
        "Our implementation is based on \n",
        "\n",
        "https://github.com/jiecaoyu/pytorch-nin-cifar10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQyEdkqpsrI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.num_classes = 100\n",
        "    self.classifier = nn.Sequential(\n",
        "                      nn.Conv2d(3, 192, kernel_size=5, stride=1, padding=2),\n",
        "                      nn.ReLU(inplace=True),\n",
        "        \n",
        "                      nn.Conv2d(192, 160, kernel_size=1, stride=1, padding=0),\n",
        "                      nn.ReLU(inplace=True),\n",
        "                      nn.Conv2d(160,  96, kernel_size=1, stride=1, padding=0),\n",
        "                      nn.ReLU(inplace=True),\n",
        "                      nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "                      nn.Dropout(0.5),\n",
        "\n",
        "                      nn.Conv2d(96, 192, kernel_size=5, stride=1, padding=2),\n",
        "                      nn.ReLU(inplace=True),\n",
        "                      nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
        "                      nn.ReLU(inplace=True),\n",
        "                      nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
        "                      nn.ReLU(inplace=True),\n",
        "                      nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
        "                      nn.Dropout(0.5),\n",
        "\n",
        "                      nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
        "                      nn.ReLU(inplace=True),\n",
        "                      nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
        "                      nn.ReLU(inplace=True),\n",
        "                      nn.Conv2d(192,  self.num_classes , kernel_size=1, stride=1, padding=0),\n",
        "                      nn.ReLU(inplace=True),\n",
        "                      nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n",
        "                      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.classifier(x)\n",
        "    x = x.view(x.size(0), self.num_classes)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1jIaKI1lmiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Rank_Activations(HookCallback):\n",
        "    \"Callback that record the mean and std of activations.\"\n",
        "    stats= []\n",
        "    \n",
        "    def on_train_begin(self, **kwargs):\n",
        "        \"Initialize stats.\"\n",
        "        super().on_train_begin(**kwargs)\n",
        "        self.stats = []\n",
        "        self.num = 0\n",
        "\n",
        "    def hook(self, m:nn.Conv2d, i:Tensors, o:Tensors)->Tuple[Rank0Tensor,Rank0Tensor]:\n",
        "        \"Take the mean and std of `o`.\"\n",
        "        self.num = self.num + 1\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "          res = o.mean().item()\n",
        "          return res\n",
        "          #print(m)\n",
        "        else:\n",
        "          #res = None\n",
        "          pass\n",
        "      \n",
        "    def on_batch_end(self, train, **kwargs):\n",
        "        \"Take the stored results and puts it in `self.stats`\"\n",
        "        if (not train): self.stats.append(self.hooks.stored)\n",
        "          \n",
        "    def on_train_end(self, **kwargs):\n",
        "        \"Polish the final result.\"\n",
        "        super().on_train_end(**kwargs)\n",
        "        self.stats = (self.stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObXs8cj2L5Io",
        "colab_type": "text"
      },
      "source": [
        "# Initializing weights and bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcYid_IfMHzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net()\n",
        "# model.cuda()\n",
        "\n",
        "pretrained=False\n",
        "if pretrained:\n",
        "    params = pickle.load(open('data/params', 'r'))\n",
        "    index = -1\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            index = index + 1\n",
        "            weight = torch.from_numpy(params[index])\n",
        "            m.weight.data.copy_(weight)\n",
        "            index = index + 1\n",
        "            bias = torch.from_numpy(params[index])\n",
        "            m.bias.data.copy_(bias)\n",
        "else:\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            m.weight.data.normal_(0, 0.05)\n",
        "            m.bias.data.normal_(0, 0.0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMfiYInuMOXv",
        "colab_type": "text"
      },
      "source": [
        "# Splitting Model into sections for discriminative learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8lfuSKOaVUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = Learner(data, model, loss_func= loss_func, opt_func = opt_func ,metrics=metrics).mixup()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBs1iGqzsuKv",
        "colab_type": "code",
        "outputId": "2689ca01-56ce-4d10-c7ab-8e8cbc207b58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learn.split([[ learn.model.classifier[0:20]], [learn.model.classifier[20] ]])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Learner(data=ImageDataBunch;\n",
              "\n",
              "Train: LabelList (50000 items)\n",
              "x: ImageList\n",
              "Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32)\n",
              "y: CategoryList\n",
              "flatfish,flatfish,flatfish,flatfish,flatfish\n",
              "Path: /root/.fastai/data/cifar100;\n",
              "\n",
              "Valid: LabelList (10000 items)\n",
              "x: ImageList\n",
              "Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32),Image (3, 32, 32)\n",
              "y: CategoryList\n",
              "flatfish,flatfish,flatfish,flatfish,flatfish\n",
              "Path: /root/.fastai/data/cifar100;\n",
              "\n",
              "Test: None, model=Net(\n",
              "  (classifier): Sequential(\n",
              "    (0): Conv2d(3, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): ReLU(inplace)\n",
              "    (2): Conv2d(192, 160, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (3): ReLU(inplace)\n",
              "    (4): Conv2d(160, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (5): ReLU(inplace)\n",
              "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (7): Dropout(p=0.5)\n",
              "    (8): Conv2d(96, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (9): ReLU(inplace)\n",
              "    (10): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (11): ReLU(inplace)\n",
              "    (12): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (13): ReLU(inplace)\n",
              "    (14): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
              "    (15): Dropout(p=0.5)\n",
              "    (16): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace)\n",
              "    (18): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (19): ReLU(inplace)\n",
              "    (20): Conv2d(192, 100, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (21): ReLU(inplace)\n",
              "    (22): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.sgd.SGD'>, momentum=0.9), loss_func=CrossEntropyLoss(), metrics=[<function accuracy at 0x7f3624024048>, <function error_rate at 0x7f3624024268>, <function top_k_accuracy at 0x7f3624024158>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/.fastai/data/cifar100'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), functools.partial(<class 'fastai.callbacks.mixup.MixUpCallback'>, alpha=0.4, stack_x=False, stack_y=True)], callbacks=[], layer_groups=[Sequential(\n",
              "  (0): Sequential(\n",
              "    (0): Conv2d(3, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): ReLU(inplace)\n",
              "    (2): Conv2d(192, 160, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (3): ReLU(inplace)\n",
              "    (4): Conv2d(160, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (5): ReLU(inplace)\n",
              "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (7): Dropout(p=0.5)\n",
              "    (8): Conv2d(96, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (9): ReLU(inplace)\n",
              "    (10): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (11): ReLU(inplace)\n",
              "    (12): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (13): ReLU(inplace)\n",
              "    (14): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
              "    (15): Dropout(p=0.5)\n",
              "    (16): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace)\n",
              "    (18): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (19): ReLU(inplace)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): Conv2d(192, 100, kernel_size=(1, 1), stride=(1, 1))\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnLRNWL9MToc",
        "colab_type": "text"
      },
      "source": [
        "# Decalring Step Dropout as Callback\n",
        "\n",
        "[Not used in this notebook]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byf9ZSraA5MZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Drop_Activations(LearnerCallback):\n",
        "  train_flag = False\n",
        "  \n",
        "  def on_train_begin(self, **kwargs):\n",
        "    \"Initialize stats.\"\n",
        "    super().on_train_begin(**kwargs)\n",
        "    self.train_flag = True\n",
        "    \n",
        "  def on_epoch_begin(self, **kwargs):\n",
        "    if (self.train_flag):\n",
        "      epoch_num = kwargs['epoch']\n",
        "      epoch_tot = kwargs['n_epochs']\n",
        "\n",
        "      child = children(self.model)[0]\n",
        "      for item in child:\n",
        "        if isinstance(item, nn.Dropout):\n",
        "          item.p = (epoch_num  / epoch_tot) * 0.5\n",
        "\n",
        "  def on_train_end(self, **kwargs):\n",
        "    \"Initialize stats.\"\n",
        "    super().on_train_begin(**kwargs)\n",
        "    self.train_flag = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXJuxCSPu7AH",
        "colab_type": "text"
      },
      "source": [
        "# Declaring Curriculum Annealing dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTiJDMFIhkNZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "eccea5f9-2d07-46e9-fd6b-084e448c2b6f"
      },
      "source": [
        "# Params as per paper\n",
        "\"\"\" \n",
        "keep_prob_input: _prob_input,   #0.9\n",
        "keep_prob_conv : _prob_conv,    #0.75\n",
        "keep_prob_fc   : _prob_fc       #0.5\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "gamma = 0.001\n",
        "p = 0.5 \n",
        "\n",
        "def schedule_dropout(x):\n",
        "    val = (1.0 - p) * np.exp(- gamma * x ) + p\n",
        "    return val\n",
        "\n",
        "#     val = -(1.-p)* np.exp(- gamma * x) + 1 # Default. \n",
        "#     This is value set to keep_prob in TF. \n",
        "#     In case of pytorch it refers to probability to make zero\n",
        "def Anneal_dropout(x):\n",
        "    val =    (1.-p)* np.exp(- gamma * x)\n",
        "    return val\n",
        "\n",
        "# 40  refers to number of epochs\n",
        "# 390 refers to (number of train images / (batch_size))\n",
        "j = []\n",
        "for i in range(0,40 * 390):  \n",
        "  j.append(Anneal_dropout(i))\n",
        "\n",
        "num_iter = 390\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(j)\n",
        "plt.ylabel('some numbers')\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZG0FBQiIpKWgkJERNJSUIiISFr/Hy+KDNa1D6VgAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MumbqAEO11hN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Curriculum_Dropout(LearnerCallback):\n",
        "    \"Curriculum Dropout Callback\"\n",
        "    count = 0\n",
        "    train_flag = False\n",
        "    \n",
        "    def on_train_begin(self, **kwargs):\n",
        "        \"Initialize stats.\"\n",
        "        super().on_train_begin(**kwargs)\n",
        "        self.train_flag = True\n",
        "        self.count = 0\n",
        "\n",
        "    def on_batch_begin(self, train, **kwargs):\n",
        "        \"Take the stored results and puts it in `self.stats`\"\n",
        "        if (train): \n",
        "          self.count += 1; \n",
        "          child = children(self.model)[0]\n",
        "          for item in child:\n",
        "            if isinstance(item, nn.Dropout):\n",
        "              item.p = Anneal_dropout(kwargs['iteration'])\n",
        "          \n",
        "    def on_train_end(self, **kwargs):\n",
        "        \"Polish the final result.\"\n",
        "        super().on_train_end(**kwargs)\n",
        "        self.count = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3usmfeiZMu31",
        "colab_type": "text"
      },
      "source": [
        "# Using the curriculum dropout in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7GkZ8ajMqjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learn.callbacks += [ Drop_Activations(learn) ]\n",
        "learn.callbacks += [ Rank_Activations(learn) ]\n",
        "learn.callbacks += [ Curriculum_Dropout(learn) ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5fz5C-KM3On",
        "colab_type": "text"
      },
      "source": [
        "# Running the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P48QhLSTKQWL",
        "colab_type": "code",
        "outputId": "500cd8c9-d0d3-4d03-bea9-6337a57a878e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs =40\n",
        "learn.fit( epochs ,  lr= (1 * base_lr , 0.1 * base_lr) , wd = (2 * w_decay , 1 *w_decay))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='36' class='' max='40', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      90.00% [36/40 21:37<02:24]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>error_rate</th>\n",
              "      <th>top_k_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.471124</td>\n",
              "      <td>4.420779</td>\n",
              "      <td>0.030900</td>\n",
              "      <td>0.969100</td>\n",
              "      <td>0.135100</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.311343</td>\n",
              "      <td>4.145817</td>\n",
              "      <td>0.063400</td>\n",
              "      <td>0.936600</td>\n",
              "      <td>0.239500</td>\n",
              "      <td>00:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.145420</td>\n",
              "      <td>3.972932</td>\n",
              "      <td>0.090100</td>\n",
              "      <td>0.909900</td>\n",
              "      <td>0.291500</td>\n",
              "      <td>00:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.001894</td>\n",
              "      <td>3.736387</td>\n",
              "      <td>0.113100</td>\n",
              "      <td>0.886900</td>\n",
              "      <td>0.357300</td>\n",
              "      <td>00:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.877210</td>\n",
              "      <td>3.529216</td>\n",
              "      <td>0.154300</td>\n",
              "      <td>0.845700</td>\n",
              "      <td>0.419600</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.735052</td>\n",
              "      <td>3.329698</td>\n",
              "      <td>0.184400</td>\n",
              "      <td>0.815600</td>\n",
              "      <td>0.464700</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.620399</td>\n",
              "      <td>3.274146</td>\n",
              "      <td>0.206100</td>\n",
              "      <td>0.793900</td>\n",
              "      <td>0.484400</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.531113</td>\n",
              "      <td>3.032808</td>\n",
              "      <td>0.260800</td>\n",
              "      <td>0.739200</td>\n",
              "      <td>0.567200</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.398856</td>\n",
              "      <td>2.869457</td>\n",
              "      <td>0.282700</td>\n",
              "      <td>0.717300</td>\n",
              "      <td>0.596000</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.278484</td>\n",
              "      <td>2.650634</td>\n",
              "      <td>0.330900</td>\n",
              "      <td>0.669100</td>\n",
              "      <td>0.641500</td>\n",
              "      <td>00:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.202642</td>\n",
              "      <td>2.586169</td>\n",
              "      <td>0.350900</td>\n",
              "      <td>0.649100</td>\n",
              "      <td>0.665800</td>\n",
              "      <td>00:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>3.104610</td>\n",
              "      <td>2.556938</td>\n",
              "      <td>0.348500</td>\n",
              "      <td>0.651500</td>\n",
              "      <td>0.667900</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>3.014765</td>\n",
              "      <td>2.347714</td>\n",
              "      <td>0.396300</td>\n",
              "      <td>0.603700</td>\n",
              "      <td>0.713800</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.963596</td>\n",
              "      <td>2.290038</td>\n",
              "      <td>0.407600</td>\n",
              "      <td>0.592400</td>\n",
              "      <td>0.734300</td>\n",
              "      <td>00:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.877850</td>\n",
              "      <td>2.235229</td>\n",
              "      <td>0.421500</td>\n",
              "      <td>0.578500</td>\n",
              "      <td>0.729000</td>\n",
              "      <td>00:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.835551</td>\n",
              "      <td>2.141058</td>\n",
              "      <td>0.445300</td>\n",
              "      <td>0.554700</td>\n",
              "      <td>0.752400</td>\n",
              "      <td>00:34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.785589</td>\n",
              "      <td>2.007154</td>\n",
              "      <td>0.472100</td>\n",
              "      <td>0.527900</td>\n",
              "      <td>0.775800</td>\n",
              "      <td>00:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.738358</td>\n",
              "      <td>2.001054</td>\n",
              "      <td>0.468700</td>\n",
              "      <td>0.531300</td>\n",
              "      <td>0.777700</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.689823</td>\n",
              "      <td>1.976157</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.525000</td>\n",
              "      <td>0.783000</td>\n",
              "      <td>00:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.634652</td>\n",
              "      <td>1.916967</td>\n",
              "      <td>0.492400</td>\n",
              "      <td>0.507600</td>\n",
              "      <td>0.790600</td>\n",
              "      <td>00:34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.618023</td>\n",
              "      <td>1.783429</td>\n",
              "      <td>0.520100</td>\n",
              "      <td>0.479900</td>\n",
              "      <td>0.817500</td>\n",
              "      <td>00:38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.559937</td>\n",
              "      <td>1.843078</td>\n",
              "      <td>0.511400</td>\n",
              "      <td>0.488600</td>\n",
              "      <td>0.805100</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.537014</td>\n",
              "      <td>1.778234</td>\n",
              "      <td>0.523900</td>\n",
              "      <td>0.476100</td>\n",
              "      <td>0.815200</td>\n",
              "      <td>00:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.517437</td>\n",
              "      <td>1.877995</td>\n",
              "      <td>0.509000</td>\n",
              "      <td>0.491000</td>\n",
              "      <td>0.803600</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.511419</td>\n",
              "      <td>1.782894</td>\n",
              "      <td>0.529800</td>\n",
              "      <td>0.470200</td>\n",
              "      <td>0.814200</td>\n",
              "      <td>00:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.468248</td>\n",
              "      <td>1.789395</td>\n",
              "      <td>0.525100</td>\n",
              "      <td>0.474900</td>\n",
              "      <td>0.818400</td>\n",
              "      <td>00:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.422069</td>\n",
              "      <td>1.700906</td>\n",
              "      <td>0.548600</td>\n",
              "      <td>0.451400</td>\n",
              "      <td>0.827400</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.419622</td>\n",
              "      <td>1.654381</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0.837800</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>2.397176</td>\n",
              "      <td>1.651654</td>\n",
              "      <td>0.562700</td>\n",
              "      <td>0.437300</td>\n",
              "      <td>0.836600</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>2.386443</td>\n",
              "      <td>1.750883</td>\n",
              "      <td>0.541300</td>\n",
              "      <td>0.458700</td>\n",
              "      <td>0.821800</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.369246</td>\n",
              "      <td>1.671527</td>\n",
              "      <td>0.557400</td>\n",
              "      <td>0.442600</td>\n",
              "      <td>0.829000</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>2.367758</td>\n",
              "      <td>1.618600</td>\n",
              "      <td>0.567000</td>\n",
              "      <td>0.433000</td>\n",
              "      <td>0.839300</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>2.309331</td>\n",
              "      <td>1.604528</td>\n",
              "      <td>0.572900</td>\n",
              "      <td>0.427100</td>\n",
              "      <td>0.842700</td>\n",
              "      <td>00:34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>2.311385</td>\n",
              "      <td>1.616985</td>\n",
              "      <td>0.570700</td>\n",
              "      <td>0.429300</td>\n",
              "      <td>0.838700</td>\n",
              "      <td>00:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>2.277174</td>\n",
              "      <td>1.625493</td>\n",
              "      <td>0.567400</td>\n",
              "      <td>0.432600</td>\n",
              "      <td>0.838300</td>\n",
              "      <td>00:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.282399</td>\n",
              "      <td>1.660736</td>\n",
              "      <td>0.559300</td>\n",
              "      <td>0.440700</td>\n",
              "      <td>0.834900</td>\n",
              "      <td>00:35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='284' class='' max='390', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      72.82% [284/390 00:23<00:08 2.2356]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWJcONPvur28",
        "colab_type": "text"
      },
      "source": [
        "# Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfjWz3CnJqTG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41976b67-a058-40e5-8530-707ee913a0a7"
      },
      "source": [
        "learn.validate()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5655344, tensor(0.5837), tensor(0.4163), tensor(0.8473)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G8pHjiluDrk",
        "colab_type": "text"
      },
      "source": [
        "# Best Results\n",
        "\n",
        "[Obtained during trial run. I hope the variation is minimal across multiple runs]\n",
        "\n",
        "\n",
        ">  train_loss --------------------------------- 2.206255\n",
        "\n",
        "> valid_loss ---------------------------------1.5655344\n",
        "\n",
        "> accuracy ---------------------------------- 0.5837 \n",
        "\n",
        "> error_rate --------------------------------- 0.4163 \n",
        "\n",
        "> top_k_accuracy ----------------------- 0.8473\n"
      ]
    }
  ]
}